{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84880b7d-3890-4d1c-8a0b-da6aa58b6521",
   "metadata": {},
   "source": [
    "## Working with Polars and Scikit-learn\n",
    "By the end of this lecture you will be able to:\n",
    "- use Polars objects to fit Scikit-learn models\n",
    "- work with categorical columns in a gradient boosting model\n",
    "- output a Polars `DataFrame` from Sklean pipeline processing tools\n",
    "\n",
    "You may need to install scikit-learn first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6964f-5e54-480c-bf00-b6223e60afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eda969-12ce-492f-b138-b3e531ad5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier,HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error,accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc960b-5222-4ab0-95c5-902a828869ff",
   "metadata": {},
   "source": [
    "In this lecture we fit simple models that aim to predict the binary `Survived` column in the Titanic data with features from a selection of other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb8870-e6a2-4df3-817c-b12710d3d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d7439-e0d1-4dbb-8460-49f54267331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_csv(csv_file)\n",
    "    .select(\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\")\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db60a2-ddeb-4fd2-87b7-c4676a7d03e5",
   "metadata": {},
   "source": [
    "## Fitting a model with Polars\n",
    "We can pass a Polars `DataFrame` and `Series` directly to Scikit-learn.\n",
    "\n",
    "Here we fit a logistic regression model using just the numeric features with some simple `null`-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362cab8-6f99-49b5-9a96-b57ef2ff5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "# Make the feature matrix and target vector\n",
    "X = df.select(pl.col(\"Age\",\"Fare\").fill_null(strategy=\"mean\"))\n",
    "y = df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f864018-ce5d-4291-b2d6-f7740850e7fb",
   "metadata": {},
   "source": [
    "We can pass this Polars `DataFrame` and `Series` directly to scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b43755-7504-42ee-8864-25ca316c1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758a4d6-78ef-4770-923e-6be2877f64ac",
   "metadata": {},
   "source": [
    "We then make a new `DataFrame` with actual labels and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22c084-73d7-46ee-a22e-7d6e26b6ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training data\n",
    "pred_df = pl.DataFrame(\n",
    "    {\n",
    "        \"label\":y,\n",
    "        \"pred\":model.predict(X)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f'Accuracy: {100*accuracy_score(pred_df[\"label\"],pred_df[\"pred\"]):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b30226-9596-4444-a309-1bb610c2a6f9",
   "metadata": {},
   "source": [
    "Note that `model.predict(X)` is a Numpy ndarray that we turn into a column in a Polars `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc32fd4-9e57-49cd-84fa-ae4e8522b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06689403-6c33-41b0-a255-95e981427a6e",
   "metadata": {},
   "source": [
    "## Categorical features in a gradient boosting model\n",
    "Typically we have to encode categorical features manually (we see how to do this in pipelines later in this lecture). However, if we use Scikit-learn's gradient boosting model `HistGradientBoostingClassifier` (or `HistGradientBoostingRegressor`) we can pass a Polars `pl.Categorical` column directly to the model without encoding it.\n",
    "\n",
    "In this example we cast the integer `Pclass` and string `Sex` column to categorical. Recall that to cast an integer column to categorical we must first cast to `pl.String`.\n",
    "\n",
    "We can pass a Polars `DataFrame` directly to `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79052b32-d5e7-4ec7-89e6-2acff5a7e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df.select(\"Pclass\",\"Age\",\"Fare\",\"Sex\",\"Survived\"),\n",
    "    test_size=0.2, \n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c543bf-4368-46dc-9a7d-c9f56d5e40d6",
   "metadata": {},
   "source": [
    "We then create our feature matrix `X` and target vector `y` by:\n",
    "- casting `Pclass` and `Sex` to categorical\n",
    "- filling `nulls` in the `Age` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec90c7-c3b9-442a-a66f-e1d12cde5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (\n",
    "    df_train\n",
    "    .select(\n",
    "        pl.col(\"Pclass\").cast(pl.String).cast(pl.Categorical),\n",
    "        pl.col(\"Sex\").cast(pl.Categorical),\n",
    "        pl.col(\"Age\").fill_null(pl.col(\"Age\").median()),\n",
    "        pl.col(\"Fare\")\n",
    "    )\n",
    ")\n",
    "y_train = df_train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ae43a-13dc-4780-ab76-2f2b5fa4ec07",
   "metadata": {},
   "source": [
    "We can now fit the model and make predic pass the categorical columns directly by passing the `categorical_features=\"from_dtype\"` argument to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1eabb5-84f3-4c34-b0b1-a6f04e4f4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grad_boost = HistGradientBoostingClassifier(categorical_features=\"from_dtype\")\n",
    "\n",
    "model_grad_boost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6980d-22f5-4254-a78a-03936c1ca3a2",
   "metadata": {},
   "source": [
    "Now we make the test feature matrix.\n",
    "\n",
    "We use `with_context` to fill `nulls` in the `Age` column of the test data with the median of the training data. We do this by:\n",
    "- `converting `df_test` to lazy mode\n",
    "- in `with_context` reference `df_train` and get the fill value as a column called `Age_median`\n",
    "- filling `nulls` in `Age` with the `Age_median` expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c897db-7ce2-4f40-80c9-9620b9977ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = (\n",
    "    df_test\n",
    "    .lazy()\n",
    "    .with_context(\n",
    "        df_train\n",
    "        .lazy()\n",
    "        .select(pl.col(\"Age\").median().alias(\"Age_median\"))\n",
    "    )\n",
    "    .select(\n",
    "        pl.col(\"Pclass\").cast(pl.String).cast(pl.Categorical),\n",
    "        pl.col(\"Sex\").cast(pl.Categorical),\n",
    "        pl.col(\"Age\").fill_null(pl.col(\"Age_median\")),\n",
    "        pl.col(\"Fare\")\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "y_test = df_test[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85cce5c-72e3-4fa3-901d-21386cab87dc",
   "metadata": {},
   "source": [
    "We now make predictions on the test data and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362abc22-4d05-4c14-82b7-ea43dfac9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pl.DataFrame(\n",
    "    {\n",
    "        \"label\":y_test,\n",
    "        \"pred\":model_grad_boost.predict(X_test)\n",
    "    }\n",
    ")\n",
    "print(f'Accuracy: {100*accuracy_score(pred_df[\"label\"],pred_df[\"pred\"]):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e4377-113c-4c8f-8c3d-533cde35da37",
   "metadata": {},
   "source": [
    "## Scikit-learn pipelines\n",
    "A more systematic way to produce ML pipelines is to use Scikit-learn's preprocessing tools: \n",
    "- `Pipeline` to compose multiple transformation steps on a column together\n",
    "- `ColumnTransformer` to run transformations on multiple columns together\n",
    "- `Pipeline` (again) to compose the pre-proprocessing and model fit/predict steps together\n",
    "\n",
    "In this example we do not cast the categorical columns to `pl.Categorical` but instead use sklearn preprocessing to create categorical features inside the preprocessing objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5736a0c-9b19-43f8-beee-4c629b5ed08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df.select(\"Pclass\",\"Age\",\"Fare\",\"Sex\",\"Survived\"),\n",
    "    test_size=0.2, \n",
    "    random_state=0\n",
    ")\n",
    "X_train,y_train = df_train.drop(\"Survived\"),df_train[\"Survived\"]\n",
    "X_test,y_test = df_test.drop(\"Survived\"),df_test[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d1022-f06c-4765-8432-4ac1a3cff560",
   "metadata": {},
   "source": [
    "We first create the `Pipeline` for the columns with numerical features. In this example we:\n",
    "- use `SimpleImputer` to fill nulls with the median value and\n",
    "- `StandardScaler` to convert numerical columns to their z-score (i.e. subtracting the mean and dividing by the standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c892b18-61f6-4f6a-be82-c6f4192e717c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"Age\", \"Fare\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e292f-d019-400f-b284-4c35af1a8d2f",
   "metadata": {},
   "source": [
    "We then create a one-hot encoding pipeline for categorical features. Note that we have to set `sparse_output=False` if we want the output as a Polars `DataFrame`. If you have a lot of sparse data you may be better off using the normal sparse matrix as the output and not using Polars in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e8273-de49-4892-b92f-0865237990b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_features = [\"Sex\", \"Pclass\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69642fff-04bf-4e67-b7dc-fc93b28f153e",
   "metadata": {},
   "source": [
    "We then make a `ColumnTransformer` to handle the preprocessing for the different types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a82079-a99a-4a57-ad9c-8b2197442067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10301f-5b3b-4ef1-9435-2cf6cb2e75e9",
   "metadata": {},
   "source": [
    "We create another `Pipeline` to handle preprocessing and model fitting. In this example we set the output to be Polars objects by calling `set_output` on the `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc80952-65ea-45fe-8a65-2f8a078fd18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_model_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "preprocess_model_pipeline.set_output(transform=\"polars\")\n",
    "\n",
    "preprocess_model_pipeline.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % preprocess_model_pipeline.score(X_test, y_test))\n",
    "\n",
    "pred_df = pl.DataFrame(\n",
    "    {\n",
    "        \"label\":y_test,\n",
    "        \"pred\":preprocess_model_pipeline.predict(X_test)\n",
    "    }\n",
    ")\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdab69-18a2-4fed-94c4-a93928aa298d",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise one\n",
    "We want to predict the tip amount in the NYC taxi database from the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92f124-c9f5-45c3-935f-f08160cdfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = \"../data/nyc_trip_data_1k.csv\"\n",
    "df_nyc = pl.read_csv(taxi_data,try_parse_dates=True)\n",
    "df_nyc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92041a7d-763f-4f23-a4bf-96d1e889ec49",
   "metadata": {},
   "source": [
    "Randomly split `df_nyc` into a train and test `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb02187-aef2-4593-8e33-5dbf6a2107a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24586bd-a3b7-4db1-827c-457ab0f41fcd",
   "metadata": {},
   "source": [
    "Create `X_train` with:\n",
    "- `VendorID` as a categorical variable\n",
    "- `trip_minutes` as the duration of the trip in minutes\n",
    "- `passenger_count`,`trip_distance`,`fare_amount` as numerical features\n",
    "\n",
    "and `y_train` with `tip_amount` as the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b62040-a53a-4ca9-9a89-2cd3298e70ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca5c367-2567-40cf-8be3-6e1f68c3fe08",
   "metadata": {},
   "source": [
    "Instantiate and fit the gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ecae7-f99e-4ce5-a178-098446c44002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b0b9267-60be-41ca-b89b-45ce81b377a9",
   "metadata": {},
   "source": [
    "Make a `DataFrame` with `actual` and `pred` columns on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20a8fe-58ca-4f1d-9106-4266d9d9aead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70383a5-44dc-4e28-9486-931e8df6113c",
   "metadata": {},
   "source": [
    "Make a scatter plot of the `actual` versus `pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b782d0-5668-4264-907d-f1ad297c0815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2252508f-ae29-4397-a3af-d5196b8a426c",
   "metadata": {},
   "source": [
    "Get the root-mean-squared error of the prediction of the tip amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe8035-ef40-4d9b-a445-af37d4c666bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df7944b-d3d7-4aef-bc0c-fa105c51394a",
   "metadata": {},
   "source": [
    "Create the test feature matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24cb58-5660-44e4-9118-e6f9dd6cdbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc93188-92a3-498f-afa8-5ae2c08f5fa0",
   "metadata": {},
   "source": [
    "Make predictions on the test `DataFrame` and make a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289b3a3-e610-448c-9c51-7b0a9c92eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d14a9d6-42eb-4c96-93d2-8acb6f336b05",
   "metadata": {},
   "source": [
    "Get the root-mean-squared error on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cd556-6062-4de9-8df3-fe05c2728825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47d4d695-338f-410e-97e1-b38cf3861acf",
   "metadata": {},
   "source": [
    "### Exercise two\n",
    "We now fit models and make predictions with a `Pipeline` approach.\n",
    "\n",
    "Begin by creating `df_nyc_train` and `df_nyc_test` by:\n",
    "- creating any feature columns and\n",
    "- splitting `df_nyc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2ae1a-7aa9-4cd0-9799-70955e25f4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5089ed5e-a1b2-4047-bb54-e00d7c8c047c",
   "metadata": {},
   "source": [
    "Now create the train/test feature matrixes and target vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb673e7-c05f-43f9-b859-4decef2705b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a236fe2-f40b-43c5-80b1-2df40c831203",
   "metadata": {},
   "source": [
    "Create a pipeline for the numerical features by:\n",
    "- imputing missing values with the median\n",
    "- scaling values by the min/max of that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10360d-d405-491c-b385-3b82b84568ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bfdcbd5-368c-48a8-9d61-c2f0b07363ca",
   "metadata": {},
   "source": [
    "Create the categorical `VendorID` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efc688-8045-4034-9e01-2df1396632b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639bd8c3-8882-468f-87da-9ea580bfb197",
   "metadata": {},
   "source": [
    "Make a `ColumnTransformer` to preprocess all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ce554-5ac2-4a3f-9011-9dccae96b81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5672c4-c8b5-48d5-90f8-3952902b2feb",
   "metadata": {},
   "source": [
    "Make a `Pipeline` to preprocess the features and do **linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bd7ab-f5c9-49e2-9b40-444a09f2920d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cd13b56-3855-4c14-b841-afaf1d74a52d",
   "metadata": {},
   "source": [
    "Make a `DataFrame` of actual and predicted values on the **test** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6971b-f9b5-4323-bef5-40b976d4ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d8d0a52-7b12-487f-94dc-c03d35a92b15",
   "metadata": {},
   "source": [
    "Get the RMSE of the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9a10e-8cb1-45ab-bb5f-75ec573c5494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d81c15f0-718a-43de-bdc1-61768389be1b",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise one\n",
    "We want to predict the tip amount in the NYC taxi database from the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2a90f-6329-4ac9-8b06-18a03972aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = \"../data/nyc_trip_data_1k.csv\"\n",
    "df_nyc = pl.read_csv(taxi_data,try_parse_dates=True)\n",
    "df_nyc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dfddf-076a-4f4c-8331-d2bd1e47df87",
   "metadata": {},
   "source": [
    "Randomly split `df_nyc` into a train and test `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231c156-598c-4e80-b906-0f3de29e5f3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df_nyc,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec80f26-5a22-4498-b471-d26e9c2c53cd",
   "metadata": {},
   "source": [
    "Create `X_train` with:\n",
    "- `VendorID` as a categorical variable\n",
    "- `trip_minutes` as the duration of the trip in minutes\n",
    "- `passenger_count`,`trip_distance`,`fare_amount` as numerical features\n",
    "\n",
    "and `y_train` with `tip_amount` as the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b559896-ff99-42d6-8b82-0d1c996b4ffe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train = (\n",
    "    df_train\n",
    "    .with_columns(\n",
    "        pl.col(\"VendorID\").cast(pl.Categorical),\n",
    "        trip_minutes = (pl.col(\"dropoff\") - pl.col(\"pickup\")).dt.total_minutes(),\n",
    "    )\n",
    "    .drop(\"pickup\",\"dropoff\",\"tip_amount\")\n",
    ")\n",
    "y_train = df_train[\"tip_amount\"]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845f326-439f-4569-a842-28bbd04cb60f",
   "metadata": {},
   "source": [
    "Instantiate and fit the gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d0310-8e04-4936-a60a-a142cd1130e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = HistGradientBoostingRegressor(categorical_features=\"from_dtype\")\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ef217-dda7-4d91-9c10-a1491bd7e89a",
   "metadata": {},
   "source": [
    "Make a `DataFrame` with `actual` and `pred` columns on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83e8ee-e3c5-4530-b9ea-ac2f5faec5da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_pred_df = pl.DataFrame(\n",
    "    {\n",
    "        \"actual\":y_train,\n",
    "        \"pred\":model.predict(X_train)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103649ae-5089-4d8f-8796-8cd4bf3cf26b",
   "metadata": {},
   "source": [
    "Make a scatter plot of the `actual` versus `pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a59e1b-f99f-44d6-880a-a615432b1e79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_pred_df.plot.scatter(\n",
    "    x=\"pred\",\n",
    "    y=\"actual\",\n",
    "    aspect='equal',\n",
    "    width=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabec93f-abff-4942-b883-738cac025e8a",
   "metadata": {},
   "source": [
    "Get the root-mean-squared error of the prediction of the tip amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cb6a7-d656-4c82-a4e8-8a64ede55322",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(train_pred_df[\"actual\"], train_pred_df[\"pred\"])\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e42651-cd04-4549-b927-513452defece",
   "metadata": {},
   "source": [
    "Create the test feature matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cecc5f-099f-455b-a788-e56f7471438d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_test = (\n",
    "    df_test\n",
    "    .with_columns(\n",
    "        pl.col(\"VendorID\").cast(pl.Categorical),\n",
    "        trip_minutes = (pl.col(\"dropoff\") - pl.col(\"pickup\")).dt.total_minutes(),\n",
    "    )\n",
    "    .drop(\"pickup\",\"dropoff\",\"tip_amount\")\n",
    ")\n",
    "y_test = df_test[\"tip_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc3554-4d69-4d53-a3e8-00465a8a4220",
   "metadata": {},
   "source": [
    "Make predictions on the test `DataFrame` and make a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf1aa2-aa14-4860-bead-4ce6b8f96654",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_pred_df = pl.DataFrame(\n",
    "    {\n",
    "        \"label\":y_test,\n",
    "        \"pred\":model.predict(X_test)\n",
    "    }\n",
    ")\n",
    "test_pred_df.plot.scatter(\n",
    "    x=\"pred\",\n",
    "    y=\"label\",\n",
    "    aspect='equal',\n",
    "    width=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc9eb2-9dae-4a8e-9f5e-8a2cbbb05703",
   "metadata": {},
   "source": [
    "Get the root-mean-squared error on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724db46-89c5-4cc0-a4fd-d962cfc89f89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(test_pred_df[\"label\"], test_pred_df[\"pred\"])\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324eccc0-e586-441f-839f-135990dd686f",
   "metadata": {},
   "source": [
    "### Solution to exercise 2\n",
    "We now fit models and make predictions with a `Pipeline` approach.\n",
    "\n",
    "Begin by creating `df_nyc_train` and `df_nyc_test` by:\n",
    "- creating any feature columns and\n",
    "- splitting `df_nyc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf23a7-9840-494f-a38c-cd2d7dfa147d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_train, df_nyc_test = train_test_split(\n",
    "    df_nyc.select(\n",
    "        \"VendorID\",\n",
    "        (pl.col(\"dropoff\") - pl.col(\"pickup\")).dt.total_minutes().alias(\"trip_minutes\"),\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"fare_amount\",\n",
    "        \"tip_amount\"\n",
    "    ),\n",
    "    test_size=0.2, \n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159f881-b137-4ffa-a228-57d160465b46",
   "metadata": {},
   "source": [
    "Now create the train/test feature matrixes and target vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061287f-35b0-4e33-a505-fb6839be7466",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train,y_train = df_nyc_train.drop(\"tip_amount\"),df_nyc_train[\"tip_amount\"]\n",
    "X_test,y_test = df_nyc_test.drop(\"tip_amount\"),df_nyc_test[\"tip_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bdca0-cefd-4e0f-b88e-082300b55e70",
   "metadata": {},
   "source": [
    "Create a pipeline for the numerical features by:\n",
    "- imputing missing values with the median\n",
    "- scaling values by the min/max of that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c16e3-dd2d-43d5-9be5-f53a443c0173",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"trip_minutes\", \"passenger_count\",\"trip_distance\",\"fare_amount\",]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"scaler\", MinMaxScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702776f-a031-414d-af08-0a28048e029a",
   "metadata": {},
   "source": [
    "Create the categorical `VendorID` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a34a5d-59e3-4501-86f9-096e218ab96d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = [\"VendorID\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709f730-1d6a-4465-9212-87f795644315",
   "metadata": {},
   "source": [
    "Make a `ColumnTransformer` to preprocess all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc458999-1c78-422a-9e9f-44f9f39cbefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6f73a-3bba-4784-b127-612c33227528",
   "metadata": {},
   "source": [
    "Make a `Pipeline` to preprocess the features and do **linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c35a2-73f0-4f53-891d-743b9152884e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preprocess_model_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", LinearRegression())]\n",
    ")\n",
    "preprocess_model_pipeline.set_output(transform=\"polars\")\n",
    "\n",
    "preprocess_model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2021-48f4-4726-8471-ac69c4e0e81a",
   "metadata": {},
   "source": [
    "Make a `DataFrame` of actual and predicted values on the **test** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e80b1-ccef-4a41-9fb7-43780e47ea46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pred_df = pl.DataFrame(\n",
    "    {\n",
    "        \"actual\":y_test,\n",
    "        \"pred\":preprocess_model_pipeline.predict(X_test)\n",
    "    }\n",
    ")\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba3dff6-a8d8-4d00-8e50-97e284abad83",
   "metadata": {},
   "source": [
    "Get the RMSE of the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af109dee-a177-4eea-a013-aa8b4b401e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(pred_df[\"actual\"], pred_df[\"pred\"])\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5bf0f-2427-4c16-b895-c52573211854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
